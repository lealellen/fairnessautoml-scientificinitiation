# -*- coding: utf-8 -*-
"""v4_Ricci_Exemplo_Completo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V46WMK3sygRTW4QYoW9vmhEvVvnZY54q

#Importando as bibliotecas que serão usadas
"""

# Commented out IPython magic to ensure Python compatibility.
### !pip install --upgrade scikit-learn==0.20.3

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier # Árvore de Decisão (modelo "explicável")


from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics # Módulo para o cálculo de medidas de desempenho
from sklearn.metrics import confusion_matrix # Função para exibir a matriz de confusão

# As próximas duas linhas servem para exibir mais dados na tela e de uma forma graficamente mais sofisticada/estruturada
# %load_ext google.colab.data_table
#pd.set_option('max_rows', 99999)

"""## Carregando o conjunto de dados
Ricci
"""

df = pd.read_csv("https://raw.githubusercontent.com/digiampietri/basesDeDados/main/ricci.csv", sep=',', encoding = 'UTF-8')
# O dataset está sem cabeçalho

# Exibindo as primeiras linhas do conjunto
df.head()

# Exibindo informações gerais do conjuntod e dados
df.info()

categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

df = pd.get_dummies(df, columns = categorical_columns, drop_first=True)

df.head()

# Adicionando coluna target
df['target'] = df['Position_Lieutenant']

# Adicionando coluna de atributo sensível para a faixa etária (1 para favorecido, 0 para os outros)
df['sensitive_attribute'] = df['Race_W']

# Separando features, rótulo e atributo sensível
x = df.drop(['target'], axis=1)
y = df['target']

sensitive_attr = df['sensitive_attribute']

# Dividir os dados em conjuntos de treino e teste
x_train, x_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split (
    x, y, sensitive_attr, test_size=0.3, random_state=42
)

# Calculando a proporção do atributo sensível
prop = x_train['sensitive_attribute'].value_counts(normalize=True)
print(prop)

# Sorteando o valor do atributo sensível de acordo com a proporção
x_train['sensitive_attribute'] = np.random.choice(prop.index, size = len(x_train), p = prop.values)

"""## Definição dos Modelos a serem utilizados"""

from sklearn.dummy import DummyClassifier # Este classificador será usado como um baseline

# Modelos que poderão ser usados como "caixa-preta"
from sklearn.ensemble import RandomForestClassifier # Floresta Aleatória
from sklearn.linear_model import LogisticRegression # Regressão Logística
from sklearn.neural_network import MLPClassifier # Rede Neural MLP
from sklearn.naive_bayes import GaussianNB # Classificador Baysiano que utiliza distribuição Gaussiana
from sklearn.neighbors import KNeighborsClassifier # KNN - K-Vizinhos mais próximos
from sklearn import svm # SVM: Máquina de Vetores de Suporte

### Modelos que serão utilizados
classificadorCaixaPreta1 = DummyClassifier(strategy='most_frequent', random_state=42, constant = None)
classificadorCaixaPreta2 = DecisionTreeClassifier()
classificadorCaixaPreta3 = RandomForestClassifier()
classificadorCaixaPreta4 = svm.SVC()
classificadorCaixaPreta5 = svm.SVC(kernel='poly')
classificadorCaixaPreta6 = LogisticRegression(max_iter=5000)
classificadorCaixaPreta7 = MLPClassifier(max_iter=5000)
classificadorCaixaPreta8 = GaussianNB()
classificadorCaixaPreta9 = KNeighborsClassifier(n_neighbors = 5)

# Criando um arranjo com o nome dos modelos (para aparecer impresso nos resultados)
#classificadoresNomes = ["Dummy", "Árvore de Decisão", "Floresta Aleatoria", "SVM (linear)", "SVM (polinomial)", "Regressão Logística", "Multilayer perceptron", "Gaussian Naive Bayes", "KNN"]
#classificadores = [classificadorCaixaPreta1, classificadorCaixaPreta2, classificadorCaixaPreta3, classificadorCaixaPreta4, classificadorCaixaPreta5, classificadorCaixaPreta6, classificadorCaixaPreta7, classificadorCaixaPreta8, classificadorCaixaPreta9]

classificadoresNomes = ["Árvore de Decisão", "Floresta Aleatoria", "SVM (linear)", "SVM (polinomial)", "Regressão Logística", "Multilayer perceptron", "Gaussian Naive Bayes", "KNN"]
classificadores = [classificadorCaixaPreta2, classificadorCaixaPreta3, classificadorCaixaPreta4, classificadorCaixaPreta5, classificadorCaixaPreta6, classificadorCaixaPreta7, classificadorCaixaPreta8, classificadorCaixaPreta9]

"""## Definição das Estratégias de Balanceamento dos Dados"""

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN
from imblearn.combine import SMOTETomek
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import CondensedNearestNeighbour
from imblearn.under_sampling import TomekLinks
from imblearn.over_sampling import BorderlineSMOTE
from imblearn.over_sampling import KMeansSMOTE

# Utilizando a técnica de Oversampling
oversample = RandomOverSampler(sampling_strategy='minority')
x_over, y_over = oversample.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica Oversampling: %i' %x_over.shape[0])

# Utilizando a técnica de Undersampling
undersample = RandomUnderSampler(sampling_strategy='majority')
x_under, y_under = undersample.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica Undersampling: %i' %x_under.shape[0])

# Utilizando a técnica de SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(random_state=42)
x_smote, y_smote = smote.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica SMOTE: %i' %x_smote.shape[0])

# Utilizando a técnica de ADASYN (Adaptive Synthetic Sampling)
adasyn = ADASYN(random_state=42)
x_adasyn, y_adasyn = adasyn.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica ADASYN: %i' %x_adasyn.shape[0])

# Utilizando a técnica de SMOTETomek
smote_tomek = SMOTETomek(random_state=42)
x_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica SMOTETomek: %i' %x_adasyn.shape[0])

# Utilizando a técnica de SMOTEENN
smote_enn = SMOTEENN(random_state=42)
x_smote_enn, y_smote_enn = smote_enn.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica SMOTEENN: %i' %x_adasyn.shape[0])

# Utilizando a técnica de Tomek Links
tl = TomekLinks()
x_tl, y_tl = tl.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica Tomek Links: %i' %x_adasyn.shape[0])

# Utilizando a técnica de Borderline
borderline_smote = BorderlineSMOTE(random_state=42)
x_border, y_border = borderline_smote.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica BorderlineSMOTE: %i' %x_adasyn.shape[0])

# Utilizando a técnica de KMeansSMOTE
kmeans_smote = KMeansSMOTE(random_state=42,k_neighbors=1)
x_kmeans, y_kmeans = kmeans_smote.fit_resample(x_train, y_train)
print('Tamanho do conjunto de treinamento com a técnica KMeansSMOTE: %i' %x_adasyn.shape[0])

# Utilizando a técnica de Condensed Nearest Neighbour
#cnn = CondensedNearestNeighbour(random_state=42)
#x_cnn, y_cnn = cnn.fit_resample(x_train, y_train)
#print('Tamanho do conjunto de treinamento com a técnica Condensed Nearest Neighbour: %i' %x_adasyn.shape[0])

#conjuntosDeTreinamento = [x_train, x_over, x_under, x_smote, x_adasyn, x_smote_tomek, x_smote_enn, x_tl, x_border, x_kmeans, x_cnn]
#rotulosDeTreinamento = [y_train, y_over, y_under, y_smote, y_adasyn, y_smote_tomek, y_smote_enn, y_tl, y_border, y_kmeans, y_cnn]

conjuntosDeTreinamento = [x_train, x_over, x_under, x_smote, x_adasyn, x_smote_tomek, x_smote_enn, x_tl, x_border, x_kmeans]
rotulosDeTreinamento = [y_train, y_over, y_under, y_smote, y_adasyn, y_smote_tomek, y_smote_enn, y_tl, y_border, y_kmeans]
conjuntosNomes = ["Original", "Oversampling", "Undersampling", "SMOTE", "ADASYN", "SMOTETomek", "SMOTEENN", "Tomek Links", "BorderlineSMOTE", "KMeansSMOTE", "CNN"]

"""# Definição das Funções para o cálculo de diferentes métricas de justiça

**Equal opportunity**

Grupos privilegiados e não privilegiados devem possuir as mesmas taxas de verdadeiros e falsos positivos.
"""

def calculate_equal_opportunity(TPR_0, TPR_1):

    #equal_op = TPR_0 = TPR_1
    equal_op = abs(TPR_1 - TPR_0)
    return equal_op

"""**Demographic parity**

A probabilidade de resultados positivos deve ser igual entre os grupos privilegiados e não privilegiados.
"""

def calculate_demographic_parity(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1):

    Total_0 = TP_0 + FP_0 + TN_0 + FN_0
    Total_1 = TP_1 + FP_1 + TN_1 + FN_1

    # Calculando a taxa de previsão positiva para cada grupo
    positive_rate_group_0 = (TP_0 + FP_0)/Total_0
    positive_rate_group_1 = (TP_1 + FP_1)/Total_1

    #return positive_rate_group_0, positive_rate_group_1
    return abs(positive_rate_group_1-positive_rate_group_0)

"""**Disparate impact (di)**

É dada pela razão entre a probabilidade de um grupo não privilegiado ter uma predição favorável e a do grupo privilegiado ter uma predição favorável.
"""

def calculate_disparate_impact(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1):

    Total_0 = TP_0 + FP_0 + TN_0 + FN_0
    Total_1 = TP_1 + FP_1 + TN_1 + FN_1

    # Calculando a taxa de previsão positiva para cada grupo
    positive_rate_group_0 = (TP_0 + FP_0)/Total_0
    positive_rate_group_1 = (TP_1 + FP_1)/Total_1

    di = positive_rate_group_0/positive_rate_group_1 if positive_rate_group_1 > 0 else 1

    return di

"""**Equalized odds**

Requer que as predições de um classificador seja independente do atributo sensível. Ambos grupos privilegiado e não privilegiado devem ter taxas equalizadas de verdadeiros positivos e falsos positivos.

"""

def calculate_equalized_odds(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1):
  FPR_0 = FP_0 / (FP_0 + TN_0)
  FNR_0 = FN_0 / (FN_0 + TP_0)

  FPR_1 = FP_1 / (FP_1 + TN_1)
  FNR_1 = FN_1 / (FN_1 + TP_1)

  equalized_odds_fpr = abs(FPR_0 - FPR_1)
  equalized_odds_fnr = abs(FNR_0 - FNR_1)

  return equalized_odds_fpr, equalized_odds_fnr

"""**Theil index**

Pode ser considerada uma medida de iniquidade entre todos os indivíduos
"""

def calculate_theil_index(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1):

    Total_0 = TP_0 + FP_0 + TN_0 + FN_0
    Total_1 = TP_1 + FP_1 + TN_1 + FN_1

    positive_rate_group_0 = (TP_0 + FP_0) / Total_0 if Total_0 > 0 else 0
    positive_rate_group_1 = (TP_1 + FP_1) / Total_1 if Total_1 > 0 else 0

    positive_rates = [positive_rate_group_0, positive_rate_group_1]

    positive_rates = np.array(positive_rates)
    mean_rate = np.mean(positive_rates)

    # Evitar log de zero
    with np.errstate(divide='ignore', invalid='ignore'):
        theil_index = (np.sum(positive_rates * np.log(positive_rates / mean_rate) if mean_rate > 0 else 1 ) / len(positive_rates))   if len(positive_rates) > 0 else 1

    if (np.isnan(theil_index)):
      return 1
    return theil_index

"""**Predictive equality**

Também conhecida como “False positive error rate balance”, corresponde ao balanceamento dos falsos resultados positivos entre os dois grupos.
"""

def calculate_predictive_equality(FP_0, TN_0, FP_1, TN_1):

    FPR_0 = FP_0 / (FP_0 + TN_0)
    FPR_1 = FP_1 / (FP_1 + TN_1)

    predictive_equality = abs(FPR_0 - FPR_1)

    return predictive_equality, FPR_0, FPR_1

"""**Average odds difference (aod)**

É dado pela média das diferenças de falso-positivo e verdadeiro-positivo entre os dois grupos.
"""

def calculate_average_odds_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):
    # Calculando a taxa de verdadeiros positivos (TPR) para cada grupo
    TPR_0 = TP_0 / (TP_0 + FN_0) if (TP_0 + FN_0) != 0 else 0
    TPR_1 = TP_1 / (TP_1 + FN_1) if (TP_1 + FN_1) != 0 else 0

    # Taxa de falsos positivos (FPR) para cada grupo
    FPR_0 = FP_0 / (FP_0 + TN_0) if (FP_0 + TN_0) != 0 else 0
    FPR_1 = FP_1 / (FP_1 + TN_1) if (FP_1 + TN_1) != 0 else 0

    # Diferença média das taxas
    aod = 0.5 * (abs(TPR_0 - TPR_1) + abs(FPR_0 - FPR_1))

    return aod

"""**Disparate mistreatment**

É verificado na taxa de erro de classificação de grupos durante o processo de treinamento.
"""

def calculate_disparate_mistreatment(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

  # Taxa de falsos positivos (FPR) para cada grupo
  FPR_0 = FP_0 / (FP_0 + TN_0) if (FP_0 + TN_0) != 0 else 0
  FPR_1 = FP_1 / (FP_1 + TN_1) if (FP_1 + TN_1) != 0 else 0

  disp_mist = (abs(FPR_0-FPR_1))/(FPR_0+FPR_1) if (FPR_0+FPR_1) != 0 else 0

  return disp_mist

"""**Non-parity**

Diferença absoluta na média de taxas preditas entre os dois grupos, adaptação do conceito de statistical parity.
"""

def calculate_non_parity(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):
    # Calculando a taxa de verdadeiros positivos (TPR) para cada grupo
    TPR_0 = TP_0 / (TP_0 + FN_0) if (TP_0 + FN_0) != 0 else 0
    TPR_1 = TP_1 / (TP_1 + FN_1) if (TP_1 + FN_1) != 0 else 0

    non_parity = (abs(TPR_1-TPR_0))/(TPR_1+TPR_0) if (TPR_1+TPR_0) != 0 else 0

    return non_parity

"""**Positive predictive parity**

Dentre um grupo favorável, as predições devem apontar resultados iguais independente do atributo sensível.

"""

def calculate_positive_predictive_parity(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

  # Calculando PPV para cada grupo
  PPV_0 = TP_0 / (TP_0 + FP_0) if (TP_0 + FP_0) != 0 else 0
  PPV_1 = TP_1 / (TP_1 + FP_1) if (TP_1 + FP_1) != 0 else 0

  positive_predictive_parity = abs(PPV_1 - PPV_0) / (PPV_1 + PPV_0) if (PPV_1 + PPV_0) != 0 else 0

  return positive_predictive_parity

"""**Positive class balance**

Dentre um grupo favorável, deve existir igualdade entre indivíduos com variáveis sensíveis diferentes.
"""

def calculate_positive_class_balance(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

  # Calculando a proporção de casos positivos para cada grupo
  P_0 = TP_0 / (TP_0 + FN_0) if (TP_0 + FN_0) != 0 else 0
  P_1 = TP_1 / (TP_1 + FN_1) if (TP_1 + FN_1) != 0 else 0

  # Calculando o Positive Class Balance
  positive_class_balance = abs(P_1 - P_0) / (P_1 + P_0) if (P_1 + P_0) != 0 else 0

  return positive_class_balance

"""**Negative class balance**

Dentre um grupo não favorável, deve existir igualdade entre indivíduos com variáveis sensíveis diferentes.
"""

def calculate_negative_class_balance(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

    # Calculando a proporção de casos negativos para cada grupo
    N_0 = TN_0 / (TN_0 + FP_0) if (TN_0 + FP_0) != 0 else 0
    N_1 = TN_1 / (TN_1 + FP_1) if (TN_1 + FP_1) != 0 else 0

    # Calculando o Negative Class Balance
    negative_class_balance = abs(N_1 - N_0) / (N_1 + N_0) if (N_1 + N_0) != 0 else 0

    return negative_class_balance

"""**Counterfactual fairness**

É uma condição que diz que a decisão deveria ser a mesma no mundo real e em um mundo “paralelo” em que o indivíduo tem o atributo sensível “favorável”.

"""

from sklearn.metrics import accuracy_score

def calculate_counterfactual_fairness(model, X, sensitive_feature):
    # X é o DataFrame de características
    # sensitive_feature é o nome da coluna da característica sensível

    # Fazer previsões para o conjunto original
    y_pred_original = model.predict(X)

    # Criar um conjunto de dados contra-factual
    X_counterfactual = X.copy()

    # Inverter o valor da característica sensível no dataset contra-factual
    X_counterfactual[sensitive_feature] = X[sensitive_feature].apply(lambda x: 1 if x == 0 else 0)


    # Fazer previsões para o conjunto contra-factual
    y_pred_counterfactual = model.predict(X_counterfactual)

    # Calcular a taxa de mudança nas decisões
    changes = np.sum(y_pred_original != y_pred_counterfactual)
    total = len(y_pred_original)

    # Calculando a métrica de Counterfactual Fairness
    counterfactual_fairness = changes / total

    return counterfactual_fairness

"""**Equal opportunity difference (eod)**

É dado pela diferença da taxa de verdadeiros positivos entre os grupos privilegiado e não-privilegiado.
"""

def calculate_equal_oportunity_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

    equal_op_dif = abs(TPR_1 - TPR_0)
    return equal_op_dif

[ ]

"""**Error rate difference (erd)**

É a soma da taxa de falso-positivo e falso-negativo.
"""

def calculate_error_rate_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

  ER_0 = (FP_0 + FN_0)/(TP_0 + FN_0 + FP_0 + TN_0)
  ER_1 = (FP_1 + FN_1)/(TP_1 + FN_1 + FP_1 + TN_1)

  ERD = abs(ER_1 - ER_0)

  return ERD

"""**Recall difference(rd)**

Diferença das métricas de revocação entre os grupos privilegiado e não privilegiado.

"""

def calculate_recall_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

  TPR_0 = TP_0/(TP_0 + FN_0)  if (TP_0 + FN_0) != 0 else 0
  TPR_1 = TP_1/(TP_1 + FN_1)  if (TP_1 + FN_1) != 0 else 0

  recall_diff = abs(TPR_1 - TPR_0)

  return recall_diff

"""**Difference in positive proportions (dpp)**

É definida pela diferença entre as proporções de predições positivas entre os grupos privilegiado e não privilegiado.
"""

def calculate_difference_positive_proportions(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

    proportion_0 = (TP_0 + FP_0) / (TP_0 + FN_0 + TN_0 + FP_0)
    proportion_1 = (TP_1 + FP_1) / (TP_1 + FN_1 + TN_1 + FP_1)

    dpp = abs(proportion_1 - proportion_0)

    return dpp

"""**Difference in rejection rates (drr)**

É a diferença das razões de verdadeiros positivos e negativos preditos das classes privilegiada e não privilegiada.
"""

def calculate_difference_in_rejection_rates(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1):

#  DRR_0 = (FP_0 + TN_0) / (TP_0 + FN_0 + FP_0 + TN_0)
#  DRR_1 = (FP_1 + TN_1) / (TP_1 + FN_1 + FP_1 + TN_1)

  DRR_0 = (TN_0) / (FN_0 + TN_0) if (FN_0 + TN_0) != 0 else 0
  DRR_1 = (TN_1) / (FN_1 + TN_1) if (FN_1 + TN_1) != 0 else 0

  DRR = abs(DRR_1 - DRR_0)

  return DRR

"""## Execução dos testes
** Para cada classificador e conjunto de treinamento ** são calculadas as diferentes métricas de avaliação e justiça.
"""

X = x
contClas = -1
#classificadores2 = classificadores[:2]
classificadores2 = classificadores
for clas in classificadores2:
  contClas = contClas + 1
  contConj = -1
  for conj in conjuntosDeTreinamento:
    contConj = contConj + 1
    print("\n### Execução atual:\t", classificadoresNomes[contClas], "\t", conjuntosNomes[contConj], "\t ###")
    modelo = clas.fit(conjuntosDeTreinamento[contConj],rotulosDeTreinamento[contConj])
    y_pred = modelo.predict(x_test)
    #print("Acurácia:",metrics.accuracy_score(y_test, y_pred))
    report=metrics.classification_report(y_test,y_pred)
    #print(report)
    #print(confusion_matrix(y_test, y_pred))




    # Definições e cálculos iniciais
    positive_label = 1

    # Calcular a matriz de confusão geral
    conf_matrix = confusion_matrix(y_test, y_pred, labels=[positive_label, 0])

    # Calcular a matriz de confusão para cada grupo
    cm_group_0 = confusion_matrix(
        y_test[np.array(sensitive_test) == 0],
        y_pred[np.array(sensitive_test) == 0],
        labels=[positive_label, 0]
    )
    cm_group_1 = confusion_matrix(
        y_test[np.array(sensitive_test) == 1],
        y_pred[np.array(sensitive_test) == 1],
        labels=[positive_label, 0]
    )

    # Extraindo valores da matriz de confusão para o Grupo 0
    TP_0 = cm_group_0[1, 1]  # Verdadeiros Positivos para o Grupo 0
    FN_0 = cm_group_0[1, 0]  # Falsos Negativos para o Grupo 0
    FP_0 = cm_group_0[0, 1]  # Falsos Positivos para o Grupo 0
    TN_0 = cm_group_0[0, 0]  # Verdadeiros Negativos para o Grupo 0

    # Extraindo valores da matriz de confusão para o Grupo 1
    TP_1 = cm_group_1[1, 1]  # Verdadeiros Positivos para o Grupo 1
    FN_1 = cm_group_1[1, 0]  # Falsos Negativos para o Grupo 1
    FP_1 = cm_group_1[0, 1]  # Falsos Positivos para o Grupo 1
    TN_1 = cm_group_1[0, 0]  # Verdadeiros Negativos para o Grupo 1

    # Calculando taxas de verdadeiros positivos e falsos positivos
    TPR_0 = TP_0 / (TP_0 + FN_0) if (TP_0 + FN_0) != 0 else 0
    TPR_1 = TP_1 / (TP_1 + FN_1) if (TP_1 + FN_1) != 0 else 0
    FPR_0 = FP_0 / (FP_0 + TN_0) if (FP_0 + TN_0) != 0 else 0
    FPR_1 = FP_1 / (FP_1 + TN_1) if (FP_1 + TN_1) != 0 else 0

    # Imprimindo resultados
    print("Matriz de Confusão Geral:")
    print(conf_matrix)
    print("\nMatriz de Confusão para o Grupo 0:")
    print(cm_group_0)
    print("\nMatriz de Confusão para o Grupo 1:")
    print(cm_group_1)

    print("\nTaxas de Verdadeiros Positivos:")
    print(f"Grupo 0: {TPR_0}")
    print(f"Grupo 1: {TPR_1}")

    print("\nTaxas de Falsos Positivos:")
    print(f"Grupo 0: {FPR_0}")
    print(f"Grupo 1: {FPR_1}")

    print("\n")

    equal_opportunity = calculate_equal_opportunity(TPR_0, TPR_1)
    print(f"Equal opportunity:\t{equal_opportunity}")

    #positive_rate_group_0, positive_rate_group_1 = calculate_demographic_parity(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1)
    #print(f"Demographic parity (0):\t{positive_rate_group_0}")
    #print(f"Demographic parity (1):\t{positive_rate_group_1}")
    demographic_parity = calculate_demographic_parity(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1)
    print(f"Demographic parity:\t{demographic_parity}")

    disparate_impact = calculate_disparate_impact(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1)
    print(f"Disparate impact:\t{disparate_impact}")

    equalized_odds_fpr, equalized_odds_fnr = calculate_equalized_odds(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1)
    print(f"Diferença na Taxa de Falsos Positivos (FPR):\t{equalized_odds_fpr}")
    print(f"Diferença na Taxa de Falsos Negativos (FNR):\t{equalized_odds_fnr}")

    theil_index = calculate_theil_index(TP_0, FP_0, TN_0, FN_0, TP_1, FP_1, TN_1, FN_1)
    print(f"Índice de Theil:\t{theil_index}")

    predictive_equality, FPR_0, FPR_1 = calculate_predictive_equality(FP_0, TN_0, FP_1, TN_1)
    print(f"Predictive Equality:\t{predictive_equality}")

    aod = calculate_average_odds_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Average Odds Difference (AOD):\t{aod}")

    disparate_mistreatment = calculate_disparate_mistreatment(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Disparate mistreatment:\t{disparate_mistreatment}")

    nonparity = calculate_non_parity(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Non-Parity:\t{nonparity}")

    ppp = calculate_positive_predictive_parity(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Positive Predictive Parity:\t{ppp}")

    positive_class = calculate_positive_class_balance(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Positive Class Balance:\t{positive_class}")

    negative_class = calculate_negative_class_balance(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Negative Class Balance:\t{negative_class}")

    fairness_metric = calculate_counterfactual_fairness(modelo, X, 'sensitive_attribute')
    print(f"Counterfactual Fairness Metric:\t{fairness_metric:.2f}")

    equal_oportunity_dif = calculate_equal_oportunity_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Equal opportunity difference:\t{equal_oportunity_dif}")

    error_rate_difference = calculate_error_rate_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Error rate difference:\t{error_rate_difference}")

    recall_difference = calculate_recall_difference(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Recall difference:\t{recall_difference}")

    difference_positive_proportions = calculate_difference_positive_proportions(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Positive proportions difference:\t{difference_positive_proportions}")

    difference_in_rejection_rates = calculate_difference_in_rejection_rates(TP_0, FN_0, FP_0, TN_0, TP_1, FN_1, FP_1, TN_1)
    print(f"Difference in rejection rates:\t{difference_in_rejection_rates}")

print("\n\n")